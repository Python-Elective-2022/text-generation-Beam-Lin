{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "a9fe9cbe-d8aa-4429-aa29-18df3df1017f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-17 06:19:15--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.12.138, 142.251.12.100, 142.251.12.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.12.138|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0rqgi9pt9e68kfgcu0fc4bes0dntsa1r/1679033925000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=43bab54c-1382-48ca-9182-3d7b1963368d [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-17 06:19:17--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/0rqgi9pt9e68kfgcu0fc4bes0dntsa1r/1679033925000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=43bab54c-1382-48ca-9182-3d7b1963368d\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.200.132, 2404:6800:4003:c00::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  35.4MB/s    in 2.0s    \n",
            "\n",
            "2023-03-17 06:19:20 (35.4 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "bce2f84f-db51-43c7-e94b-e5af93ae6e09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "a76d3a09-d2bd-4657-bb1b-8b9750a34b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "840ffe41-b73a-4b52-eff9-f30022538152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 16s 75ms/step - loss: 5.9819 - accuracy: 0.0267\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 5.4420 - accuracy: 0.0358\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.3705 - accuracy: 0.0333\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 5.3196 - accuracy: 0.0419\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 5.2505 - accuracy: 0.0419\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1836 - accuracy: 0.0474\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 5.1164 - accuracy: 0.0434\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 5.0432 - accuracy: 0.0510\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9616 - accuracy: 0.0499\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 4.8662 - accuracy: 0.0701\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.7605 - accuracy: 0.0838\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.6573 - accuracy: 0.0878\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5510 - accuracy: 0.1034\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4466 - accuracy: 0.1236\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3351 - accuracy: 0.1387\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 4.2420 - accuracy: 0.1382\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1385 - accuracy: 0.1569\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.0442 - accuracy: 0.1791\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.9520 - accuracy: 0.1922\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8613 - accuracy: 0.2048\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7757 - accuracy: 0.2185\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6933 - accuracy: 0.2346\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6139 - accuracy: 0.2442\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5348 - accuracy: 0.2704\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.4701 - accuracy: 0.2755\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.3861 - accuracy: 0.2977\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3113 - accuracy: 0.3113\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2464 - accuracy: 0.3244\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1874 - accuracy: 0.3380\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1342 - accuracy: 0.3567\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.0583 - accuracy: 0.3658\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0082 - accuracy: 0.3890\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9414 - accuracy: 0.3991\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.8815 - accuracy: 0.4137\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.8306 - accuracy: 0.4299\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.7754 - accuracy: 0.4455\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.7230 - accuracy: 0.4531\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.6884 - accuracy: 0.4551\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6212 - accuracy: 0.4748\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5702 - accuracy: 0.4728\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 2.5344 - accuracy: 0.4798\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4833 - accuracy: 0.4965\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4286 - accuracy: 0.5061\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3669 - accuracy: 0.5212\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 2.3174 - accuracy: 0.5298\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2790 - accuracy: 0.5444\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2346 - accuracy: 0.5499\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1971 - accuracy: 0.5605\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1672 - accuracy: 0.5610\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1351 - accuracy: 0.5661\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0715 - accuracy: 0.5787\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0219 - accuracy: 0.5918\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9856 - accuracy: 0.5974\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9947 - accuracy: 0.5918\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.9477 - accuracy: 0.5974\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9036 - accuracy: 0.6029\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8605 - accuracy: 0.6216\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8243 - accuracy: 0.6322\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8108 - accuracy: 0.6382\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.7915 - accuracy: 0.6302\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.7446 - accuracy: 0.6448\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.7082 - accuracy: 0.6529\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.6649 - accuracy: 0.6710\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.6369 - accuracy: 0.6746\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6159 - accuracy: 0.6837\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5849 - accuracy: 0.6831\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5441 - accuracy: 0.6963\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5148 - accuracy: 0.7094\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4857 - accuracy: 0.7099\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4662 - accuracy: 0.7210\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4668 - accuracy: 0.7159\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4256 - accuracy: 0.7210\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3804 - accuracy: 0.7356\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3553 - accuracy: 0.7321\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.3295 - accuracy: 0.7482\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3265 - accuracy: 0.7472\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3042 - accuracy: 0.7417\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2759 - accuracy: 0.7543\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2462 - accuracy: 0.7659\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2169 - accuracy: 0.7699\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1978 - accuracy: 0.7709\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1801 - accuracy: 0.7750\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.2292 - accuracy: 0.7492\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1902 - accuracy: 0.7644\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1481 - accuracy: 0.7704\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1133 - accuracy: 0.7876\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0964 - accuracy: 0.7896\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.0762 - accuracy: 0.7866\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0569 - accuracy: 0.7896\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.0388 - accuracy: 0.7936\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0279 - accuracy: 0.7997\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0138 - accuracy: 0.7982\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9925 - accuracy: 0.8058\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9833 - accuracy: 0.8058\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9644 - accuracy: 0.8113\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9430 - accuracy: 0.8184\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9242 - accuracy: 0.8174\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9079 - accuracy: 0.8224\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8992 - accuracy: 0.8204\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8814 - accuracy: 0.8315\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8717 - accuracy: 0.8320\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8649 - accuracy: 0.8264\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8543 - accuracy: 0.8249\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8519 - accuracy: 0.8290\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8554 - accuracy: 0.8224\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.8515 - accuracy: 0.8305\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8278 - accuracy: 0.8320\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8064 - accuracy: 0.8380\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8013 - accuracy: 0.8396\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7932 - accuracy: 0.8370\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7782 - accuracy: 0.8416\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.7578 - accuracy: 0.8471\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7489 - accuracy: 0.8436\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7503 - accuracy: 0.8436\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7280 - accuracy: 0.8522\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.7186 - accuracy: 0.8572\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7067 - accuracy: 0.8567\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7053 - accuracy: 0.8577\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6940 - accuracy: 0.8567\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6806 - accuracy: 0.8562\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6713 - accuracy: 0.8597\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6663 - accuracy: 0.8638\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6593 - accuracy: 0.8658\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6495 - accuracy: 0.8663\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6375 - accuracy: 0.8693\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6285 - accuracy: 0.8683\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6184 - accuracy: 0.8724\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6103 - accuracy: 0.8683\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6089 - accuracy: 0.8754\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6238 - accuracy: 0.8658\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6362 - accuracy: 0.8582\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6224 - accuracy: 0.8623\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6085 - accuracy: 0.8628\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5907 - accuracy: 0.8688\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5725 - accuracy: 0.8784\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5627 - accuracy: 0.8789\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5611 - accuracy: 0.8829\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5488 - accuracy: 0.8819\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5479 - accuracy: 0.8799\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5440 - accuracy: 0.8799\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5347 - accuracy: 0.8824\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5278 - accuracy: 0.8870\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5245 - accuracy: 0.8824\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5293 - accuracy: 0.8799\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5220 - accuracy: 0.8794\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5159 - accuracy: 0.8835\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5313 - accuracy: 0.8804\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5197 - accuracy: 0.8769\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5150 - accuracy: 0.8824\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5232 - accuracy: 0.8764\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5337 - accuracy: 0.8724\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5304 - accuracy: 0.8713\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5003 - accuracy: 0.8804\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5102 - accuracy: 0.8774\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5052 - accuracy: 0.8809\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4790 - accuracy: 0.8845\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4822 - accuracy: 0.8850\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4744 - accuracy: 0.8905\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4654 - accuracy: 0.8925\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4741 - accuracy: 0.8890\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4677 - accuracy: 0.8870\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4510 - accuracy: 0.8940\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4427 - accuracy: 0.8895\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4387 - accuracy: 0.8966\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4300 - accuracy: 0.8966\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4228 - accuracy: 0.8940\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4250 - accuracy: 0.8961\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4180 - accuracy: 0.8940\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4128 - accuracy: 0.8981\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4076 - accuracy: 0.9011\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4096 - accuracy: 0.8971\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4050 - accuracy: 0.9001\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4101 - accuracy: 0.8956\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4129 - accuracy: 0.8966\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4089 - accuracy: 0.8971\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4090 - accuracy: 0.8920\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3981 - accuracy: 0.8956\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3896 - accuracy: 0.9001\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3860 - accuracy: 0.8976\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3803 - accuracy: 0.9016\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3782 - accuracy: 0.8991\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3774 - accuracy: 0.9031\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3783 - accuracy: 0.8991\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3878 - accuracy: 0.8991\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3732 - accuracy: 0.9001\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3735 - accuracy: 0.9011\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3703 - accuracy: 0.8996\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3616 - accuracy: 0.9036\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3642 - accuracy: 0.9031\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3582 - accuracy: 0.9036\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3555 - accuracy: 0.9001\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3596 - accuracy: 0.8996\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3575 - accuracy: 0.9021\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.3567 - accuracy: 0.8986\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3526 - accuracy: 0.9046\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3568 - accuracy: 0.8996\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3465 - accuracy: 0.9011\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3535 - accuracy: 0.9036\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3490 - accuracy: 0.9036\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3399 - accuracy: 0.9051\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "0c237abf-bf2f-4cf2-f98d-595bd1a8260f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "39a77508-3763-4557-d3b0-01d7195ff46f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 651ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "im feeling chills of warning a knew how before sleep song chiquitita chiquitita sleep song chiquitita chiquitita have have you eyes can intention intention pride kind eyes around song chiquitita have you eyes happy never chiquitita you song chiquitita chiquitita song chiquitita chiquitita tell back and me humdehumhum fine me humdehumhum shining above you can eyes feeling come she come you leaves me fingers night scars eyes velvet eyes velvet eyes hate us would would would intention intention bedumbbedumbdumb wanted am song chiquitita chiquitita how song chiquitita have have have have chiquitita you of of take andante andante andante andante andante andante andante\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}